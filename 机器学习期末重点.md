- 线性回归模型
	- 损失函数、正则化项的影响（加上后梯度下降的公式）
		- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412122157921.jpg)note: x_0 = 1，为了卷积方便
		- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412122158291.jpg)
	- 杂项
		- 特征缩放
			- 均值归一化![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412122206385.jpg)
		- 学习率α
			- 若α太小，收敛过程很慢。模型每次更新的步长很小，需要更多的迭代才能找到最优解。
			- 若α太大，损失函数J(θ)可能在每次迭代中并没有持续减少，甚至可能出现震荡，导致无法收敛。
	- 正态方程
		- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412122214724.jpg)最优解
	- 正态方程 vs 梯度下降

| 梯度下降      | 正态方程          |
| --------- | ------------- |
| 需要选择学习率   | 无需选择学习率       |
| 需要多次迭代    | 无需迭代          |
|           | 需要计算(X^TX)^-1 |
| 当n较大时依然可行 | 当n较大时非常慢      |


- 过拟合和欠拟合
	- 概念
		- 过拟合是指模型在训练数据上表现很好，但在新的、未见过的数据上表现不佳的现象。这通常是因为模型过于复杂，过度地拟合了训练数据中的噪声和异常点。
		- 欠拟合（Underfitting）是指机器学习模型在训练数据上表现不佳，并且在测试数据上也无法达到满意的性能，也就是说模型没有很好地捕捉到数据中的规律和特征。
	- 在曲线上的表现
		- 
	- 各自的解决方法
		- 过拟合
			- 减少特征数量：去掉那些不重要的或者可能带来噪声的特征
			- 正则化
				- 线性回归
				- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412161655488.jpg)
				- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412161655232.jpg)
				- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412161701392.jpg)
				- 逻辑回归
				- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412161702801.jpg)
				- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412161703581.jpg)
			




- 决策树
	- 构建算法，递归 ，停止条件（分而治之）
		- 递归返回的3种情况
			- 当前结点包含的样本全属于一个类别，无需划分；
			- 当前==属性==集为空，或是所有样本在所有属性上取值相同，无法划分
			- 当前结点包含的==样本集合==为空，不能划分
		- 信息熵
			- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412161709973.jpg)
			- Ent(D) 的值越小，则D的纯度越高
			- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412161710617.jpg)
			- 

	- 最优的划分曲线（3种）
	- 剪枝（预剪枝、后剪枝，两者的优缺点）
		- 剪枝是决策树预防“过拟合”的主要手段
		- 预剪枝 (pre-pruning): 提前终止某些分支的生长
		- 后剪枝 (post-pruning): 生成一棵完全树，再“回头”剪枝
		- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412161732754.jpg)


| 预剪枝              | 后剪枝                     |     |
| ---------------- | ----------------------- | --- |
| 减低了过拟合的风险        |                         |     |
| 减少了决策树的训练时间和测试时间 | 泛化能力往往优于预剪枝决策树          |     |
| 带来了欠拟合的风险        | 欠拟合风险很小                 |     |
|                  | 训练时间开销比未剪枝决策树和预剪枝决策树大得多 |     |


- 二分类问题
	- 逻辑回归
		- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/20241215214423127.png)

	- 多分类如何转换为二分类问题（one vs all）
		- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/20241215214229195.png)训练阶段
		- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/20241215214313372.png)预测阶段


- 模型评估
	- 常见指标：精确度、查准率、查全率
		- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412132154830.jpg)

	- 常见曲线的绘制：PR、ROC、代价曲线（预测不同类型的错误对应的惩罚）
		- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412132155973.jpg)ROC曲线
		- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/20241215230038904.png)
		- PPT中提到的ROC绘制方法
			- 给定 m + 个正例和 m - 个反例，根据预测结果对样本排序，在 (0,0) 处标记一个点。
			- 设前一个标记点为 (x,y)，当当前为真正例时，则对应标记点的坐标为 (x,y + 1/m + )；当当前为假正例时，则对应标记点为 (x + 1/m -,y)，然后用线段连接相邻点即可。
		- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412132156355.jpg)ROC曲线和PR曲线的关系
		- 代价曲线
			- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412161645204.jpg)
			- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412161646789.jpg)



- 聚类
	- ~~k-means算法（过程）~~
		- 步骤：
			- 先定义一共有多少个类/簇(==clusters==)
			- 将每个簇心(==clusters center==)随机定在一个点上
			- 将每个数据点关联到最近簇中心所属的簇上（欧式距离）
			- 对于每个簇找到其所有关联点的中心点（取每个点坐标的平均值）
			- 将上述点变为新的簇心
			- 不停重复，直到每个簇的点不变
	- ~~怎样评估聚类效果~~
		- 类内距离最小化（聚类凝聚性）
			- 目的是让同一个聚类内的点尽量靠近，增加聚类的紧凑性
			- 表现为聚类中心和点之间的平均距离尽可能小
		- 类间距离最大化（聚类分离性）
			- 不同聚类之间的点应该尽可能远，以便更好地分离
			- 表现为不同聚类中心之间的距离尽可能大
- 推荐系统
	- 基于内容过滤
		- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412161740787.jpg)

	- 基于协作过滤
		- 协同过滤是一种基于[用户行为](coco://sendMessage?ext=%7B%22s%24wiki_link%22%3A%22https%3A%2F%2Fm.baike.com%2Fwikiid%2F4915376383893789640%22%7D)数据（如用户对商品的评分、浏览记录、购买行为等）来发现用户兴趣偏好，进而进行个性化推荐的方法。
	- 怎样解决冷启动的问题 
		- 冷启动问题是指在协同过滤系统中，当新用户加入或者新商品上架时，由于缺乏足够的用户 - 商品交互数据（如评分、购买等），导致系统难以对这些新用户或新商品进行准确的个性化推荐。
		- ![image.png](https://cdn.jsdelivr.net/gh//2784343633/ImageUpload/picgo/202412161739080.jpg)

- 异常检测算法
1. **基于独立特征分布的异常检测算法**
    - **原理**：假设各特征独立，为每个特征拟合[概率分布](coco://sendMessage?ext=%7B%22s%24wiki_link%22%3A%22https%3A%2F%2Fm.baike.com%2Fwikiid%2F6525924666865961890%22%7D)（如正态分布），计算数据点各特征概率乘积来判断是否异常。
    - **优点**：计算简单、易理解和解释，对低维且特征独立的数据有效。
    - **缺点**：忽略特征相关性、高维数据效果差、对分布假设敏感。
2. **基于多元高斯分布的异常检测算法**
    - **原理**：将所有特征视为整体，用多元高斯分布概率密度函数计算每个数据点概率，概率低的为异常点。
    - **优点**：考虑特征相关性，在高维数据中有优势。
    - **缺点**：计算复杂、对数据量和分布要求高、参数估计困难。
